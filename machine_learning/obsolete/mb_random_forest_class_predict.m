function [y_fit votes error_trace proximities] = mb_random_forest_class_predict(forest, X, y)
%MB_RANDOM_FOREST_CLASS_PREDICT given a random forest and input data,
%predict the set of class labels for the data
%   [random_forest] = mb_random_forest_class_predict(forest, X)
%
%   X:      N x d matrix of input data, where each row is a
%           datapoint consisting of d input variables
%
%   forest: Random forest structure as generated by
%   MB_RANDOM_FOREST_CLASS_TRAIN
%
% Outputs:
%   y_fit:  Set of class labels predicted for each data point (note these
%           are currently returned as text labels in an Nx1 cell string
%           array. To convert to numeric values call str2double(y_fit),
%           which will produce an Nx1 vector of equivalent numeric values
%
%   votes:  Votes for each class for each data point (note by dividing by
%           the number of trees in the forest these can be used to
%           constructed output probabilities (as opposed to hard labels)
%
% Example:
%
% Notes:
%
% See also: MB_RANDOM_FOREST_CLASS_TRAIN MB_TREE_PREDICT
%
% Created: 13-Oct-2009
% Author: Michael Berks 
% Email : michael.berks@postgrad.man.ac.uk 
% Phone : +44 (0)161 275 1241 
% Copyright: (C) University of Manchester 

%workout number of data points and input variables
N = size(X, 1);

%Work out number of trees in forest and number of output classes
n_trees = length(forest.trees);
nclasses = forest.nclasses;
    
%pre-allocate space for class votes from each tree
votes = zeros(N, nclasses);

if nargout > 3
    proximities = zeros(N);
end
if nargin > 2
    %[y_idx, y_vals] = mb_grp2idx(y);
    %y = y_vals(y_idx);
    error_trace = zeros(n_trees,1);
end

for ii = 1:n_trees

    %load tree
    tree = u_load([forest.tree_root forest.tree_dir forest.trees{ii}]);
    
    %Get predictions for data from this tree
    [y_tree nodes] = mb_tree_predict(tree, X);
    
    %Assign votes for each class
    for jj = 1:nclasses
        if isnumeric(y_tree)
            votes(:, jj) = votes(:, jj) + ...
                (y_tree(:) == forest.classname(jj));
        elseif iscellstr(y_tree)
            votes(:, jj) = votes(:, jj) + ...
                strcmp(y_tree(:), forest.classname(jj));
        else
            error('Type predictions from tree should be numeric or a cell of strings');
        end
    end
    
    if nargin > 2
        %[dummy y_idx] = max(votes, [], 2); clear dummy
        %error_trace(ii) = 1 - mean(strcmp(y_vals(y_idx), y));
        
        %*del-me*
        [dummy error_trace(ii)] = calculate_roc_curve(votes(:,2)/ii, y,(-1:100)/100);
        
    end
    
    if nargout > 3
    %Compute proximity measure - i.e. count the number of times data points
    %end up at the same leaf node
        leaf_nodes = unique(nodes);
        for jj = 1:length(leaf_nodes)
            leaf = leaf_nodes(jj);
            leaf_pts = double(nodes == leaf);
            proximities = proximities + (leaf_pts * leaf_pts');
        end
    end   
end

%Get index to class with maximum votes
[dummy y_idx] = max(votes, [], 2); clear dummy

%Convert class index to class labels and compute error rate
y_fit = forest.classname(y_idx);

%If we need to calculate proximities, divide by n_trees to get measure
%between 0 and 1
if nargout > 3
    proximities = proximities / n_trees;
end
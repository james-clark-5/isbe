% Details on how a Random Forest works for either classification or regression
\label{s:rf_background}

% Then explain how sampling from the training examples and the input features, reduces correlation between trees such that a 'forest' of them produces better results

Using an ensemble of $F$ different trees (dubbed a \emph{Random Forest}~\cite{Breiman_ML01}), improves performance by reducing correlation between the outputs of the trees. Specifically, the forest produces $F$ predictions whose distribution can be used directly or averaged (using the individual measures of spread as weights if available).

The key to the Random Forest's performance is that every tree is built using randomly selected input features from randomly selected examples. For every partition at every level of the tree, only a random subset of $m < M$ input dimensions are considered rather than assessing all $M$ inputs. Similarly, $F$ bootstrap sets of $N$ examples are generated by sampling with replacement from the (finite) training data; if a generative model is available, a different training set can be synthesized for each tree without sampling.

As well as giving state of the art performance in a number of applications~\cite{Criminisi}, Random Forests also have relatively few parameters to tune and are often resistant to overtraining.

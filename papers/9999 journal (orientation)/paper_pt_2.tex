%\documentclass[a4paper]{article}
\documentclass{IEEEtran}

%\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\usepackage{nicefrac}
\usepackage{placeins}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g.,}}
\def\ie{\emph{i.e.,}}
\def\etal{\emph{et al.}}
\def\vs{\emph{vs.}}

% macros for referencing figures, tables, equations and sections
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\aref}[1]{Algorithm~\ref{#1}}
\newcommand{\emptybox}[2]{\framebox[#1][l]{\rule[#2]{0pt}{0pt}}}

% maths macros
\def\G{G}
\def\Gx{G_x}
\def\Gy{G_y}
\def\Gxx{G_{xx}}
\def\Gxxs{G_{xx}(\sigma)}
\def\Gxy{G_{xy}}
\def\Gxys{G_{xy}(\sigma)}
\def\Gyx{G_{yx}}
\def\Gyy{G_{yy}}
\def\Gyys{G_{yy}(\sigma)}
\def\Ix{I_x}
\def\Iy{I_y}
\def\Ixsqr{I_{x^2}}
\def\Iysqr{I_{y^2}}
\def\Ixx{I_{G_{xx}}}
\def\Ixxs{I_{G_{xx}}(\sigma)}
\def\Ixy{I_{G_{xy}}}
\def\Ixys{I_{G_{xy}}(\sigma)}
\def\Iyy{I_{G_{yy}}}
\def\Iyys{I_{G_{yy}}(\sigma)}
\def\Igt{I_{G_{theta}}}
\def\Iht{I_{H_{theta}}}
\def\dtcwt{DT-$\mathbb{C}$WT}

\DeclareMathOperator*{\argmax}{arg\,max}

\def\deg{\ensuremath{^\circ}}
\def\rad{\ensuremath{\text{radians}}}
\def\by{\ensuremath{\times}}

% lengths for image sizes
\newlength{\qtrcol}\setlength{\qtrcol}{0.24\columnwidth}
\newlength{\halfcol}\setlength{\halfcol}{0.48\columnwidth}

% command for adding inline comment to text
\newcommand{\comment}[1]{\textbf{[#1]}}

% define title here so headers are updated, too
\def\ttl{Analysing Curvilinear Structures in Images}
\title{\ttl}
\author{Authors}

% define path to figures
\def\figroot{./figs}
\def\figpath{\figroot}


%-------------------------------------------------------------------------
% Document starts here
\begin{document}

\tableofcontents\clearpage

\maketitle

\begin{abstract}
\comment{Rewrite this when paper content is finalised}
\end{abstract}

\input{citations.tex}


\section{Introduction}
I've deleted the old introduction for now. Instead, below is a bullet pointed list of what I think the narrative of the paper should be. First though, what I think the paper shouldn't be...

The easiest way to write the paper is a straight "here is a method for detecting linear structures and measuring low-level characteristics in images". We show results on public datasets like STARE and DRIVE and say, "look! Our detection results are better than everyone else's" and also that predicting width and orientation is more accurate than analytically estimating it.
The problems with this are:
\begin{itemize}
\item	It's hard to sell on novelty, the only really new stuff in the method is predicting orientation and width, but it's hard to motivate why people should care about that
\item	Whilst the detection results are better than previous work (at least on the last trawl of the literature) and probably statistically significantly so (although it exactly what statistical test is most appropriate), they're only a marginal improvement on what are already very good detection results (e.g. an Az of 0.98 vs 0.96). So again, the question is, so what should people care about?
\item	It wouldn't allow us to make use of all the extensive experiments we did comparing how the features work and compare to each other (or at least, it becomes hard to fit them into the narrative). After all, if all we're trying to show is our results being better than someone else's, why spend lots of the paper discussing suboptimal results from our own work. Given the time and care I spent setting up and running those experiments I'd be loath not to include them (plus think of all the heat and energy generated by the CSF - we have a moral and environmental duty to publish these results!). On a serious note, for me, these results (and the theory underpinning them) are the interesting part of the work.
\item	Also, if all we're going to do is say "look, these are the best results", then it's Gabor filters that provide them. I think the story of why you may want to consider the DT-CWT (or other separable filters) is a more subtle one than simply saying "they're more efficient", but again, it would be hard to fit this into the main narrative of the paper. Moreover, I'm keen to avoid anyone concluding, "see Gabor filters work best" and then assuming alternative methods that only consider the real parts of the filters, or apply them at a single level etc. are doing the same thing just because they're Gabor filters.
\item	Finally, we've already tried submitting a paper like this to two conferences and been rejected both times!
\item	That said, if you don't think selling the paper as outlined below is a goer (or can't think of alternative reframing) I'd be happy to write the paper in this way, I just suspect we need to lower our sights of where it might get published…
\end{itemize}

So what do I think the paper should be about?

\begin{itemize}
\item	Linear structures are important features in lots of images. We focus on medical images (vessels, fibres etc) but they're in all sorts of diverse images (road, rivers, fingerprints, cracks/fault-lines in materials)
\item	Generally previous work concentrates on detecting/segmenting structures, but other low-level characteristics such as orientation and scale are often needed in further processing and can be non-trivial to estimate
\item	Given the breadth of image types in which CLS there has naturally been a wide variety of methods employed to detect them. However in circumstances where a ground truth labelling of images is available,  recent methods have tended to converge to a standard two-stage approach in which features are extracted for each pixel (or group of pixels) in the image before a machine learning classifier is trained to distinguish between background and structure.
\item	That said, within this approach there is still huge variety, particularly in the choice of features, with a selection different filters and wavelet decompositions applied to the images, the responses of which may be combined in many weird and wonderful ways. Moreover, the tendency is to stress the uniqueness of each new method relative to previous work, making choosing a set of filters for a new (and different) set of images a complicated task.
\item	In contrast, by returning to the basic theory of filtering 2D signals, we aim to show the similarities between classes of image decompositions. We use the theory to motivate what information can be extracted to characterise structures and the efficiency with which this information can be obtained for different filter banks. In doing so we show the necessity of including information on profile symmetry, scale and orientation whilst also showing the redundancy of much further transformation of the filter responses prior to forming feature vectors.
\item	Our main contribution is to back up this theory with a comprehensive set of experiments in a variety of medical image types. In doing so, we show that by selecting filters that extract sufficient information to characterise structures and combining these with a standard random forest classifier, we are able to achieve state-of-the-art vessel segmentation performance on a well known public set of retinograms, without the need for further application specific adaptations of the method. However we stress our aim is to show not just \emph{that} a certain set of features work, but \emph{why} they work. We hope this provides a framework so that given a similar task, regardless of the particular class of image and computational budget, it is possible to make a better informed decision on the suitability of any filtering scheme for extracting reliable features.
\item	Our second contribution is to show the value of applying machine learning to predict other low-level characteristics such as a structure width and orientation as opposed to estimating them analytically from the filter responses. This requires little extra computation (because the feature vectors have already been extracted from the images to perform segmentation) and results in significantly more accurate predictions. Again we show this experimentally on various classes of medical images ***?? and show how the improved accuracy produces a follow-on improvement in the measurement of clinical biomarkers for nailfold capillaroscopy ??*** (we haven't done this last part yet but we probably could)
\item	We also note that whilst we do not make any claims on the novelty of our machine learning approach (indeed we posit that provided the features are right, any suitably trained non-linear learning algorithm may be used), we do introduce tweaks to standard random forests that improved how sample points are selected from the data and take account of the non-euclidean nature of orientation as a regression output. Finally we also discuss how the output of random forests can be used to estimate a distribution over the predictions (as has been done in other settings, but in a CLS setting), that may in turn provide added information in further processing.
\end{itemize}

The paper is set out as follows: in section \sref{s:output_labels} we describe how we label the output variable for pixels in the training images using a ground truth segmentation. In section \sref{s:filtering} we describe the desired properties of a filter bank for decomposing images across scale and orientation, and  in section \sref{s:composing_features} we discuss the various ways the raw filter responses may be combined into feature vectors at each pixel in an image. In section \sref{s:learning_methods} we describe the random forests we have used as classifiers/regressors and introduce the changes we have made from their standard application. Section \sref{s:experiments} comprises our results, including experiments on synthetic data to reinforce the theory discussed in section 3a, a comprehensive comparison across the three classes of medical images for of all compositions of feature vector discussed in section 3b and finally a comparison of our results to previously published work on a public database of retinograms. Discussion and conclusions follow in sections \sref{discussion} and \sref{conclusions}.


\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c@{}}
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_test} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_segmentation_gabor_inv.png} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_orientation_mag} \\
%\includegraphics[height=0.15\textheight]{\figpath/retina/002_abs_error} \\
(a) & (b) & (c) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Estimating orientation in retinography: %
(a) input image; %
(b) segmentation of vessels by random forest classification of Gabor features; %
(c) orientation (indicated by colour) estimated using random regression over Gabor~features.}
\label{f:first_pic}
\end{figure}

\section{Target Output Labels}
\label{s:output_labels}
Our approach to analyzing and understanding images containing linear structure is to use statistical learning methods to recognize patterns present in training data in order to predict useful information in previously unseen images. More specifically, we focus on two tasks: detecting linear structures in the image; and measuring the orientation of linear structures. Though the input image features are common across both tasks, the output label we wish to predict is task-dependent.

\subsection{Detecting Linear Structure}
\label{s:output_detection}

One of the most basic tasks in image interpretation is to determine which pixels correspond to linear structures such as vessels, spicules and road, and which pixels correspond to background data in which we have no interest. Throughout we refer to the former as CLS pixels, and the latter as background pixels.

In our definition of CLS, each structure has a finite width and can be unambiguously differentiated from the image background (regardless of what other structures may be present in the background). As such, for a given class of images (retinograms, aerial maps etc), it is possible to obtain training images in which each pixel can either be labelled as belonging to a CLS or belonging to background. By convention, we assign the CLS a class label $\mathcal{C}=1$ and the background $\mathcal{C}=0$. Typically these labels are based on a ground truth segmentation that has been generated by an expert human observer

To segment an unseen image, we train a statistical classifier to predict an output label $C_{est}(p) \in [0, 1]$ at each pixel $p$, such that $C_{est}(p)$ represents the probability that $p$ belongs to a CLS.
%
%\subsection{Classifying Linear Structure}
%\input{why/classify_lines/in_general}%
%\input{why/classify_lines/in_mammograms}%
%\input{output_labels/label_classification}%
%
We define the error between the estimated probability of CLS and the ground truth using the cross entropy,
%
\begin{equation}
E(p)   = [1 - \mathcal{C}_{gt}(p)] \,    \mathcal{C}_{est}(p) +
						  \mathcal{C}_{gt}(p)  \, [1-\mathcal{C}_{est}(p)].
\label{e:detection_error}
\end{equation}


\subsection{Measuring Orientation}
\label{s:output_orientation}
%
Orientation is a continuous variable and it is appropriate to estimate its value by regression. Orientation, however, violates a common assumption in that it does not live in a Euclidean space; adding $2\pi$ radians gives the same orientation, for example. Also, orientation (unlike \emph{direction}) is only defined over half the circle; orientations exactly $\pi$ radians apart are also considered to be identical.

A different representation is therefore needed, and one that respects these two conditions uses a unit vector in the complex plane where the underlying angle at each pixel, labelled at $\theta_{gt}(p)$, is doubled such that two orientations exactly $\pi$ radians apart are mapped to the same complex value~\cite{Mardia_Jupp_00},
%
\begin{equation}
	t_{gt}(p) = \cos 2\theta_{gt}(p) + i\sin 2\theta_{gt}(p).
\end{equation}
%
% Difference between two orientations
Representing orientation as a complex vector also allows us to define the distance between an estimated value, $t_{est}$, and the ground truth, $t_{gt}$, as
%
\begin{equation}
	E(p) = \frac{|\angle(t_{gt}(p) \cdot t_{est}^*(p))|}{2},
\label{e:orientation_error}
\end{equation}
%
\noindent where $t^*$ denotes the complex conjugate of $t$. This error measure accounts for the circular nature of orientation in a principled way.

% Interpreting the mean vector and its magnitude
Taking the mean over a set of orientations gives a complex vector whose angle is twice the average orientation over the set, and whose magnitude,
%
\begin{equation}
D = \left| \frac{\sum{t_k}}{N} \right|,
\label{e:angular_dispersion}
\end{equation}
%
\noindent defines the spread -- known as the \emph{angular dispersion} -- of the samples in the set~\cite{Mardia_Jupp_00}. By definition, $D$ reaches a maximum of $1$ when all $t_k$ are equal, and a minimum of $0$ when orientations are distributed uniformly about the circle or when the sample consists of pairs exactly $\pi$ radians apart.

\subsection{CLS width}
\label{s:output_width}
\comment{Do we need this now?}

When the ground truth is skeletonised to compute target orientation labels, we can also apply a distance transform to label the width of each CLS pixel, which we refer to as $w_{gt}(p)$. If desired, $w_{gt}(p)$ is straightforward to regress directly -- in which case it may be preferable to label only the skeletonised (\ie centreline) pixels as CLS in the detection task. In unseen images, predictions of CLS centrelines may then be thinned, and a combination of predicted orientation and width used to estimate a complete segmentation of CLS.

However, we do not use that approach in this work, although $w_{gt}(p)$ is used during the evaluation of detection and orientation prediction to asses how performance varies a function of CLS width.

\comment{width estimation results can be included results for these if we think it adds to the paper?}


%\clearpage

\section{Input Image Features}
\label{s:filtering}

Having defined target outputs that we intend to predict, in this section we describe what input features we will base our learning on. Our basic approach is to convolve an image with a bank of filters, generating a set of responses at each pixel that represent local image structure.

To complete the two tasks defined in the previous section, this requires that the set of responses for any given CLS pixel should be distinguishable from the responses of any background pixel; and the filters should be directionally selective to predict orientation. Whilst this may seem trivial to state, we note that the former condition is often not satisfied by many of the filter banks previously used to detect CLS.

We now describe each of the filter banks tested in this paper.

%\comment{First, we explain how to get the response to an orientation sensitive filter at any orientation. Then we show how some filters permit alternative representations, such as magnitude and phase. Finally, we discuss options for pooling filter responses over translation, orientation and scale to create a complete signature for an image feature.}

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gx} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gxx} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gxy} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gxx-Gyy} \\
(a) & (b) & (c) & (d) \\
\noalign{\smallskip}
%
\includegraphics[width=0.2\columnwidth]{figs/filtering/mono_b} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/mono_hx} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/dt_cwt_r4} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/dt_cwt_c4} \\
(e) & (f) & (g) & (h) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{(a)~First derivatives $\Gx = \Gy^T$; (b-d)~Second derivatives, $\Gxx = \Gyy^T$, $\Gxy$; and $\Gxx-\Gyy$; (e,f)~Monogenic signal filters $B$ and $h_x = h_y^T$; (g,h)~Real and complex responses of the \dtcwt~ $15^\circ$ subband.}
\label{f:filters}
\end{figure}


\subsection{Gaussian derivatives}
\label{s:filtering_secondderivs}

\subsubsection{Orientation-Sensitive Filtering}
Derivatives of a Gaussian kernel are probably the most commonly used set of filters for linear structure detection (certainly within the context of vessel segmentation in medical images). Throughout, we use $G \equiv G(x,y;\sigma)$ to refer to a 2-dimensional Gaussian kernel with zero mean and standard deviation $\sigma$, and use subscripts to denote derivatives of $G$ with respect to a particular direction.

The first derivatives of a Gaussian kernel, taken separately with respect to the horizontal ($G_x$) and vertical ($G_y$) axes, are odd filters (\ie~$G_x(-x,y) = -G_x(x,y)$ and $G_y(x,-y)=-G_y(x,y)$) that can be used to estimate the strength and direction of the image gradient at each pixel. As such, they are commonly used to detect asymmetric image features (\ie~edges) that are associated with a large image gradient in the direction perpendicular to the edge's orientation. However, a basic property of CLS is that they have approximately symmetric profile and so the first derivative responses of a pixel at the centre of a CLS will be near zero.

Accordingly, when CLS are the structures of interest, it is typical to use second derivatives. Differentiating $G_x$ and $G_y$ again both horizontally and vertically, produces three filters, $\Gxx$, $\Gyy$ and $\Gxy = \Gyx$, that can be combined to generate a filter,
%
\begin{equation}
G_\theta = \Gxx \cos^2(\theta) + \Gyy \sin^2(\theta) + \Gxy \sin(2\theta),
\label{e:secondderivs_filter}
\end{equation}
%
with even symmetry that resembles a bar or ridge feature at any arbitrary direction $\theta$. However, we need never actually compute $G_\theta$ and apply it to an image, as its response $I_{G_{\theta}}$ may be computed directly from the responses to the separable filters, $\Ixx$, $\Iyy$ and $\Ixy$, as
%
\begin{equation}
I_{G_\theta} = \Ixx \cos^2(\theta) + \Iyy \sin^2(\theta) + \Ixy \sin(2\theta),
\label{e:secondderivs_response}
\end{equation}
%
\noindent a property known as \emph{steerability}~\cite{Freeman_Adelson_TPAMI91}. This response function has four stationary points in the range $[0,2\pi)$, occurring at
%
\begin{equation}
\theta = \frac{1}{2} \tan^{-1}\left( \frac{2\Ixy}{\Ixx-\Iyy} \right).
\label{e:secondderivs_orientation}
\end{equation}
%
Two of these points, $\hat{\theta}$ and $\hat{\theta}+\pi$, correspond to the directions in which the filter is aligned with the feature and the \emph{absolute} value of the response is maximal; the other two points correspond to the two perpendicular directions ($\hat{\theta}^\perp$ and $\hat{\theta}^\perp+\pi$).

Because, however, the maximal absolute response may correspond to either a maximum or minimum (depending on whether the underlying feature is light-on-dark or vice versa) the only way to find out which of the two perpendicular directions has maximal absolute value is to evaluate the response at both and choose the direction corresponding to the larger absolute value. As a result, estimating orientation from second-order derivatives becomes a nonlinear problem.\footnote{Note this approach is often reformulated by creating a Hessian matrix~\cite{missing} with the $xx$ and $yy$ derivatives on the lead diagonal and the $xy$ derivatives on the opposing diagonal. Solving this matrix produces eigenvectors with direction $\hat{\theta}$ and $\hat{\theta}^\perp$ and magnitude $I_{\hat{\theta}}$ and $I_{\hat{\theta}^\perp}$}

\comment{Efficient approximations to computing second-order derivatives of the Gaussian can be achieved through Haar-like approximations to the derivative filters~\cite{Bay_etal_CVIU08} or by approximating the Gaussian filter and applying local finite differences~\cite{Kovesi_DICTA10} *could omit?}.

\subsubsection{Combining Information from Even and Odd Filters}

Second derivatives are used on the assumption that when steered to match the orientation of a CLS, the response will be large, whilst the response in the perpendicular direction will be near-zero. Thus CLS can be distinguished from flat backgrounds (where both the aligned and perpendicular responses are near-zero) or circular blob-like structures (where both responses are large). Moreover, \eref{e:secondderivs_orientation} provides an elegant solution for determining this orientation analytically although, as noted previously and justified experimentally in \sref{s:experiments_1}, we prefer a learning based approach to predicting orientation from the filter responses.

In either an analytical or learning approach, however, there are two problems in using second derivatives alone. Firstly, a strong edge in the image will produce an ``echo'' response that cannot be distinguished from the response at the centre of a CLS. This may seem an arbitrary construct but it is easy find examples in real data. The edge of the optic disc in a retinogram (\fref{f:drive_odd_even}), for example, is often misclassified as a vessel, whilst similar artifacts may occur at the edges of lesions or near the pectoral muscle in mammograms.

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c@{}}
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_optic} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_optic_g2d_inv} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_optic_g12d_inv} \\
%\includegraphics[height=0.15\textheight]{\figpath/retina/002_abs_error} \\
(a) & (b) & (c) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Detecting vessels in retinography: %
(a) Magnified region containing optic disk; %
(b) Segmentation using only even filter responses as features: the false positive predictions
at the edge of the disk have similar strength to neighbouring vessels; %
(c) Segmentation using odd and even filter responses: false positives are still present, but at a
much lower strength to nearby vessels;
}
\label{f:drive_odd_even}
\end{figure}

Secondly, due to noise in the image, the symmetric profile of a CLS may be disrupted to the extent that not only does equation \eref{e:secondderivs_orientation} produce inaccurate estimations of orientation, the responses themselves hold no useful information for a machine learning algorithm to exploit. Again, this can be seen clearly in real data, particularly in structures that have a width of only one or two pixels, such as the smallest vessels in retinograms.

The first problem tells us that it is not enough to have only filters designed to match the shape profile of CLS if the filters responses cannot distinguish other structures in the image background. The second problem says that we cannot rely on the assumptions we make about CLS in real images corrupted by noise.

Combining both ideas motivates us to use a filter bank that more generally represents \emph{any} image feature, rather than one particular structure type. Our goal then is not to make a priori assumptions about the filters responses, but simply to ensure the set of filter responses at a pixel of any one structure type is different from those of a pixel belonging to any other structure. It is then up to our chosen machine learning algorithm to match the various patterns of responses present in the training data to the output measure of interest.

An intuitive solution is to supplement the \emph{even} second derivative filters with the \emph{odd} first derivatives. We prefer, however, to use the Hilbert transform of the second derivatives to provide filters with odd symmetry~\cite{Freeman_Adelson_TPAMI91}. In this method, four separable basis filters
%
\begin{equation}
H_a = 0.9780(2.2540x + x^3)G
\label{e:secondderivs_hilberta}
\end{equation}
\begin{equation}
H_b = 0.9780(0.7515 + x^2)yG
\label{e:secondderivs_hilbertb}
\end{equation}
\begin{equation}
H_c = 0.9780(0.7515 + y^2)xG
\label{e:secondderivs_hilbertc}
\end{equation}
\begin{equation}
H_d = 0.9780(2.2540y + y^3)G
\label{e:secondderivs_hilbertd}
\end{equation}
%
may be used to generate a steered response
\begin{align}
   I_{H_\theta} = ~~~ &I_{H_a} \cos^3(\theta) \nonumber \\
   								 -~ &I_{H_b} 3\cos^2(\theta)\sin(\theta) \nonumber \\
   								 +~ &I_{H_c} 3\cos(\theta)\sin^2(\theta) \nonumber \\
   								 -~ &I_{H_d} \sin^3(\theta).
\label{e:secondderivs_hilberts}
\end{align}

\subsubsection{Separating Phase and Magnitude}

For the Gaussian derivatives, the advantage of using $I_{H_\theta}$ (as opposed to a first derivative response) is that it may be combined with $I_{G_\theta}$ to the overall magnitude of the response at a given orientation and scale, and its corresponding phase shift
%
\begin{equation}
M_\theta = \sqrt{I_{G_\theta} + I_{H_\theta}}
\label{e:secondderivs_mag}
\end{equation}
%
\begin{equation}
\psi_\theta = \tan^{-1}\left( \frac{I_{G_\theta}}{I_{H_\theta}} \right)
\label{e:secondderivs_phase}
\end{equation}
%
These have an intuitive interpretation where the value of $M_\theta$ at any pixel tells us whether a structure with orientation $\theta$ is present, whilst $\psi_\theta$ tells us the structure's profile (in the direction $\theta^{\perp}$), as it varies from a valley ($\psi_\theta=\nicefrac{\pi}{2}$), to a step ($\psi_\theta=0,\pi$), to a ridge ($\psi_\theta=\nicefrac{-\pi}{2}$).

This gives us three basic forms of Gaussian responses to use as features:
\begin{enumerate}
  \item The raw responses to the separable filters at each scale:
  \begin{equation*}
    \{I_{G_xx}, I_{G_xy}, I_{G_yy}, I_{H_a}, I_{H_b}, I_{H_c}, I_{H_d}\}
  \end{equation*}

  \item Steered responses at a number of discrete angles (typically $N_\theta=6$), spread evenly around the circle:
  \begin{equation*}
    \{I_{G_\theta}, I_{H_\theta}\}, \; \theta = [ \nicefrac{i\pi}{N_\theta} ]
  \end{equation*}
  \item Steered responses, converted to magnitude and phase:
  \begin{equation*}
    \{M_\theta, \psi_\theta\}, \; \theta = [ \nicefrac{i\pi}{N_\theta} ]
  \end{equation*}
\end{enumerate}

\subsubsection{Combining information Over Scale}

To account for CLS with different widths, the separable filters are applied at multiple scales, typically by varying $\sigma$ in octave scales (\ie increasing powers of two), and the resulting responses (converted into one of the three forms above) concatenated into a single feature vector. Alternatively, responses at different scales may be generated by successively downsampling the image by a factor of two (\ie decimating) and then applying a filter with fixed $\sigma$. The decimated responses must then be interpolated to the resolution of the original pixel grid to produce and equivalent feature vector.

The latter approach will be much faster to compute, but the interpolated responses at coarser scales may be less robust (\ie more susceptible to noise) than those computed from the full-size image. We test both approaches in \sref{s:experiments_2}.

\subsection{Gabor filters}
\label{s:filtering_gabor}
A Gabor filter pair consists of one sine and one cosine function, in two dimensions, windowed with a Gaussian kernel:

\begin{equation}
W_{re}(x,y;\lambda,\theta,\sigma,\gamma) = \exp \left( -\frac{u^2 + \gamma^2 v^2}{2\sigma^2} \right)
    \cos \left( \frac{2\pi u}{\lambda} \right)
\label{e:gabor_real}
\end{equation}

\begin{equation}
W_{im}(x,y;\lambda,\theta,\sigma,\gamma) = \exp \left( -\frac{u^2 + \gamma^2 v^2}{2\sigma^2} \right)
    \sin \left(  \frac{2\pi v}{\lambda} \right)
\label{e:gabor_imag}
\end{equation}
where
\begin{align}
u = x\cos\theta + y\sin\theta \\
%
v = -x\sin\theta + y\cos\theta
\label{e:gabor_xy}
\end{align}
%
Thus $\sigma$ again controls the filter's scale, $\gamma$ varies the shape of the Gaussian kernel (we always use a circular kernel with $\gamma=1$ although in other work elliptical kernels have been used~\cite{Dabbah_etal_MICCAI10}), and $\lambda$ controls the frequency of the sine waves. Here, we adopt the usual approach of fixing the ratio between $\sigma$ and $\lambda$ at each scale, and set $\lambda = 2.35\sigma$ as recommend in ~\cite{Ayres_Rangayyan_JEI07}.

The even filter $W_{re}$ and odd filter $W_{im}$ differ in phase by $90\deg$, and thus like a Gaussian second derivative $G_\theta$ and its Hilbert partner $H_\theta$, their responses $I_{re}$ and $I_{im}$ can be combined into a magnitude and phase,
%
%
\begin{equation}
M_\theta = \sqrt{I_{re} + I_{im}}
\label{e:secondderivs_mag}
\end{equation}
%
\begin{equation}
\psi_\theta = \tan^{-1}\left( \frac{I_{re}}{I_{im}} \right)
\label{e:secondderivs_phase}
\end{equation}
%
%
Because Gabor filters are directionally sensitive, and can also recover phase information, they are a popular choice of filter in image processing applications~\cite{Daugman_TASSP88}. However, like Gaussian derivatives, where they have also been used to detect CLS, it is often only the even filter $W_{re}$ that is used. For the same reason we match $G_\theta$ with $H_\theta$, we recommend using both $W_{re}$ and $W_{im}$, and show experimentally the advantages of doing so in \sref{s:experiments_2}.

An advantage of Gabor filters is that, by varying the ratio of $\lambda$ to $\sigma$, they allow us to control the directional selectivity \comment{what does this mean? the sharpness of the peak?} of each filter in a manner not available to Gaussian derivatives. Comparing $I_{G_\theta}$ steered to six angles spaced equally about the circle (\fref{f:oriented_responses}a), with that of $I_{re}$ for six identically oriented Gabor filters (\fref{f:oriented_responses}b) for a pixel on a simple line feature rotated through $180\deg$, the drop-off from maximum to minimum response is much faster for each Gabor sub-band. This suggests Gabor filters may be a more discriminative predictor of CLS orientation.

Unlike Gaussian derivatives, however, Gabor filters are neither separable nor steerable, though methods to approximate steerability have been explored~\cite{Teo_1987,Perona_PAMI95}. Therefore, they must be applied exhaustively over a discrete set of orientations at each scale. This makes them much more computationally expensive, and also requires more memory (if all responses are to be stored simultaneously).

In an attempt to get the best of both worlds, we next consider a filtering scheme, that offers similar directional selectivity to Gabor filters, at a fraction of the computational cost, and has so far been unexploited in applications analysing CLS.

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.3\columnwidth]{figs/filtering/g2d_oriented_response} &
\includegraphics[width=0.3\columnwidth]{figs/filtering/gabor_oriented_response} &
\includegraphics[width=0.3\columnwidth]{figs/filtering/dt_oriented_response} \\
(a) & (b) & (c) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Oriented sub-band response as a function of CLS orientation (a) Gaussian; (b) Gabor; (c) \dtcwt{}.}
\label{f:oriented_responses}
\end{figure}


\subsection{The Dual-tree Complex Wavelet Transform}
\label{s:filtering_dtcwt}

The Dual-Tree Complex Wavelet Transform (\dtcwt{}~\cite{Kingsbury_ACHA01}) was developed to overcome the lack of directional selectivity and shift-invariance that is inherent in discrete wavelet transforms, whilst remaining fully invertible and efficient [maintaining the efficiency of decimation \comment{which is?}]

As a filtering scheme, the \dtcwt{} combines separable high and low-pass filters to produce, at a given scale, sub-bands \comment{`filters'?} at six orientations: $\pm 15\deg$, $\pm 45\deg$ and $\pm 75\deg$. Each sub-band contains complex coefficients, the real and imaginary parts of which are equivalent to the responses \comment{really?} from a pair of filters that differ in phase by $90\deg$~(\fref{f:filters_dtcwt}), although neither filter has perfect odd or even symmetry.

As with Gabor and Gaussian responses, at each scale and orientation we can choose to use either the raw responses (\ie the real and imaginary parts of the interpolated complex coefficients) directly as features or convert the pair of responses into magnitude and phase.

Here we use Q-Shift filters and include the additional filters to reduce the wave frequencies of the $\pm 45\deg$ sub-bands so that they lie closer to those at $\pm 15\deg$ and $\pm 75\deg$, as well as adjusting all six sub-bands so that the phase at the centre of the impulse response of each wavelet is zero~\cite{Kingsbury_ECSP06}.

Examining the response of each sub-band to a line rotated about the circle (\fref{f:oriented_responses}c), the sub-bands have a similar directional selectivity to the Gabor filters (\fref{f:oriented_responses}a). However, unlike the Gaussian and Gabor filters, the \dtcwt{} sub-bands do not have perfect rotation symmetry. The effect this has on features constructed from \dtcwt{} coefficients is explored in \sref{s:experiments_2}.

% Multiresolution filtering
To compute filter responses at different scales, the image is repeatedly downsampled by a factor of two in every axis before applying the same separable filter pairs again. This makes the \dtcwt{} extremely fast to compute. Moreover, downsampling means the representation has a redundancy of just 4:1, making it feasible to store the decomposition of even large images (by contrast a full Gabor representation, computed at six orientations over 5 scales has a redundancy of 60:1).

To get the response for every scale at a given location on the original pixel grid, the decimated filter responses are interpolated using the method defined in~\cite{Anderson_etal_ICIP05}. Because the transformation is designed to be shift-invariant, these interpolations are well-defined and theoretically valid.

\subsection{The Monogenic Signal}
\label{s:filtering_monogenic}

\comment{When you put forward the theoretical arguments - there really isn't much to call for the Monogenic Signal}

The final filtering scheme we include is also designed to efficiently compute magnitude, phase and orientation at multiple scales in an image. The Monogenic Signal~\cite{Felsberg_Sommer_TSP01} achieves this using three filters: one even band-pass filter $B$ (\fref{f:filters}e), and an odd quadrature pair of filters (\fref{f:filters}f)
\begin{equation}
h_x(x,y) = \frac{x}{f(x,y)}
\label{e:mono_hx}
\end{equation}

\begin{equation}
h_y(x,y) = \frac{y}{f(x,y)}
\label{e:mono_hy}
\end{equation}
where
\begin{align}
f(x,y) = 2\pi(x^2 + y^2)^{\frac{3}{2}}
\label{e:mono_f}
\end{align}

These filters are combined to compute a magnitude~($M$), phase~($\psi$) and orientation~($\theta$) at every location in the image:

\begin{align}
M       &= \sqrt{{I_B}^2 + {I_{hx}}^2 + {I_{hy}}^2}
\label{e:monogenic_amplitude} \\
%
\psi	  &= \tan^{-1}\left[ \frac{I_B}{\sqrt{{I_{hx}}^2 + {I_{hy}}^2}} \right]
\label{e:monogenic_phase} \\
%
\theta  &= \tan^{-1}\left[ \frac{I_{hy}}{I_{hx}} \right]
\label{e:monogenic_orientation}
\end{align}

Scale is controlled by the bandpass filter $B$, for which we use log-Gabor filter, defined in the frequency domain as

\begin{equation}
B(\omega) = \exp \left( -\frac{ \log(\nicefrac{r}{f_o})^2 } {2 \log(\sigma_o)^2} \right)
\label{e:monogenic_bandpass}
\end{equation}
%
where $f_o$ is the centre frequency of the filter, and $\sigma_o$ controls the bandwidth (we use $\sigma_o=0.65$, and double $f_o$ at each scale to obtain bands of approximately 1 octave).

The efficiency with which the Monogenic Signal computes local measures of magnitude, phase and orientation relative to a filter bank such as Gabor, has made it popular in several image processing tasks~\cite{missing}, including an attempt to detect CLS in mammograms~\cite{wai}.

However, whilst the Monogenic Signal combines both odd and even filters, the even filter $B$ is isotropic and therefore has no directional sensitivity. Consequently all orientation information comes from the odd filters $h_x$ and $h_y$, such that orientation cannot not be recovered for symmetric image features such as CLS (where both the numerator and denominator in \eref{e:monogenic_orientation} will be near zero).

\subsection{Composing feature vectors from filter responses}
\label{s:composing_features}
Thus far we have described the four filter banks included in our experiments, and the basic form of features we can extract from each set of filter responses. Below we consider some additional modifications we can apply to these basic feature vectors.

\subsubsection{``Folding'' phase}
\label{s:composing_features_complex}
When filter responses have been combined as phase $\psi$ and magnitude $M$, $\psi$ is computed using the four-quadrant arc-tangent, and is thus defined over $[-\pi, \pi)$. However, if an image is rotated through $180\deg$, the phase $\psi$ at any pixel becomes $\pi - \psi$ (\ie~in polar form, it is reflected about the vertical axis). Thus if we define $\psi^*$ as

\begin{equation}
\psi^* =
\begin{cases}
      \pi - \psi    & \mathrm{if}~\psi \in [-\pi, \nicefrac{-\pi}{2}) \\
      \psi          & \mathrm{if}~\psi \in [\nicefrac{-\pi}{2}, \nicefrac{\pi}{2}] \\
      \pi - \psi    & \mathrm{if}~\psi \in (\nicefrac{\pi}{2}, \pi]
   \end{cases}
\label{e:folding_phase}
\end{equation}
%
\noindent then feature vectors using $M$ and $\psi$ are invariant to $180\deg$ rotations, as theoretically desirable for orientation.

\subsubsection{Rotational Invariance}
\label{s:composing_features_rotation}
Given a set of responses at orientations spread evenly over the circle, a common approach is to rearrange the dimensions of the feature vector such that the orientation that produces maximum response occupies the same dimension in each feature vector, with the remaining orientations permuted circularly so that the relative order is unchanged. The idea is to produce feature vectors with rotational invariance thus collapsing the size of the feature space (proportional to the number of discrete orientations in the filter bank) and making it easier for the classifier to do its job.

With responses over multiple scales, we can choose either to allow the responses in each scale to shift independently or choose a single maximum orientation (\eg from the scale that produces maximum response) to use in all scales. Here we take the former approach although we have experimented with both and found little difference in performance.

\subsubsection{Pooling Over Translation}
In addition to compiling responses across scale and orientation at each pixel, we can also pool the responses from neighbouring pixels. We have experimented with more exotic sampling schemes in which we interpolate responses in a circular pattern about each pixel, but have found that in practice simply sampling a $3\times3$ window of responses, with the labelled pixel in the centre of the window, provides most benefit. Of course this increases the dimension of the feature vectors nine-fold, but with machine learning algorithms (such as random forests) designed to cope with large dimensional features and plenty of training data (which is nearly always the case in this set up given each pixel in an image is a sample and thus even a small set of images typically contain millions of samples) this needn't be a problem.

\subsection{Other filter banks}

\label{s:filtering_extras}
Finally we acknowledge that there are of course many further filter banks we do not test in this paper, and for which an exhaustive comparison of results is unfeasible. However, we show \emph{how} and \emph{why} the properties of the filter banks we do test affect performance. Thus, given a set computational cost, we can make an informed choice of suitable filter bank for any given data.

We also show that a filter bank selected given these general criteria can produce features that outperform features handcrafted for a particular application.

One interesting concept we do not test is the idea of learning an optimal set of arbitrary filters for a given set of images, as in~\cite{missing}. However, we believe that such an approach is only beneficial if the filters are optimised with respect to the task they need to perform (in our case and in~\cite{missing}, separating the responses for CLS and background pixels within a classifier) and cannot see how optimising with respect to some other task (such as reconstructing the image in a maximally sparse way) is intrinsically a desirable thing to do. That said a comparison with the results in~\cite{missing} would be desirable if quantitative results on the DRIVE and STARE datasets were made available.

\section{Statistical Learning Methods}
\label{s:learning_methods}
Having defined a label to assign to each pixel (either its class $\mathcal{C}$, or orientation vector $t_{gt}$), and defined several options for composing feature vectors that represent the local structure associated with the pixel, it remains to choose a learning method that can associate patterns in the input features to the output labels, across a training set of images.

In this work, we consider three machine learning algorithms of varying complexity: a linear classifier; a boosted classifier; and a Random Forest~\cite{Breiman_ML01} each of which is described below.

\comment{Actually, I've omitted the experiments that compared classifier as I wasn't whether they added value? They can be added back in if necessary however, although we would need to choose where?}

%\subsection{Linear Classification}
%\label{s:learning_linear}
%\input{methods/machine_learning/linear_regression/regression_linear.tex}

%\subsection{Logistic Classification}
%\label{s:learning_logistic}
%\input{regression_logistic}

%\subsection{Boosted Learning}
%\label{s:learning_boosted}
%\input{methods/machine_learning/boosting/regression_boosted.tex}%

\subsection{Decision Trees}
\label{s:learning_trees}

% Start by explaining how a single tree predicts a label (either discrete or continuous) from an input
Decision Trees -- a well-established and popular approach to Machine Learning -- are easy to understand and implement, and are capable of learning complex, nonlinear relationships over large numbers of variables (with absolute scales that are incommensurate) at a modest computational cost.

A typical training algorithm for a classification and regression tree (CART~\cite{missing}) iteratively partitions the input space in order to optimize some splitting measure until a termination criterion is satisfied. The input feature vectors may then be discarded and the tree used as a lookup table where the output labels assigned to each leaf can be retained (and later sampled) or replaced by a description of their distribution (\eg~mean and variance) to reduce memory use. Both representations permit multimodal distributions (which is useful at points where lines cross or bifurcate) and a measure of confidence in the prediction of a previously unseen example.

\comment{Measure of confidence?}

%Splitting criterion
When determining how to partition the input space, all distinct partitions (based on a threshold applied to an input feature) of the examples at a given node are ranked based on a measure of `goodness of split'. As an example, consider the average variance over the two potential partitions in question: the variance is minimized at the point where the data set is split with the minimum overlap.

%Termination criterion
The examples assigned to every leaf are split repeatedly until some termination criterion is satisfied. One approach is to continue splitting the data until every leaf contains samples with exactly the same label (a `pure' leaf node). Leaves can later be merged -- a process known as `pruning' -- for efficient lookup at run-time. We have found (as have others~\cite{Criminisi_MICCAI11}), however, that it is computationally more efficient and more robust to stop based on measure of spread within the two proposed leaves (we use 0.05\% of the total variance over all output labels). One further advantage of avoiding pure leaf nodes is that each leaf contains several training samples such that we can estimate the distribution over output labels for every leaf, thus providing a measure of confidence in the prediction that varies over the input space.

\subsection{Random Forests}
\label{s:learning_forest}

% Details on how a Random Forest works for either classification or regression
\label{s:rf_background}

% Then explain how sampling from the training examples and the input features, reduces correlation between trees such that a 'forest' of them produces better results

Using an ensemble of $F$ different trees (dubbed a \emph{Random Forest}~\cite{Breiman_ML01}), improves performance by reducing correlation between the outputs of the trees. Specifically, the forest produces $F$ predictions whose distribution can be used directly or averaged (using the individual measures of spread as weights if available).

The key to the Random Forest's performance is that every tree is built using randomly selected input features from randomly selected examples. For every partition at every level of the tree, only a random subset of $d < D$ input dimensions are considered rather than assessing all $D$ inputs. Similarly, $F$ bootstrap sets of $N$ examples are generated by sampling with replacement from the (finite) training data; if a generative model is available, a different training set can be synthesized for each tree without sampling.

As well as giving state of the art performance in a number of applications~\cite{Criminisi}, Random Forests also have relatively few parameters to tune and are often resistant to overtraining.

% Details on how a Random Forest works specifically for detection
%
During classification, an unseen feature vector is classified independently by each tree in the forest; each tree casts a unit class vote, and the most popular class can be assigned to the input vector. Alternatively, the proportion \comment{distribution?} of votes assigned to each class can be used to provide a probabilistic labelling of the input vector.

% Points specific to estimating orientation with a Random Forest
When using a Random Forest as a regressor to predict orientation, we must take special care to ensure that the sample statistic (\eg~variance) used to partition the training data is appropriate. One statistic that does respect the circular nature of orientation is the angular dispersion, defined in~\eref{e:angular_dispersion} and so it is a natural choice for choosing an optimal partition of the sample.

When using a pruned tree, we can replace the samples at each leaf by a histogram that captures  any multimodal properties of the output (where lines cross, for example). Alternatively, we can replace the samples at each leaf by their summary statistics to save memory. In the case of orientation, the mean of the complex values gives both the average orientation (the angle of the mean vector) and the angular dispersion of the sample (the magnitude of the mean vector). This therefore provides a measure of confidence in the estimate for a single tree (in contrast to when using unpruned trees where every estimate has an identical confidence of 1).

When computing the final output of the forest, we can also take the mean over all $F$ outputs to give an estimate of orientation that is weighted by the confidence in each individual prediction. This output vector will also have a magnitude in the range $[0,1]$ that indicates confidence in the overall prediction produced by the forest.

Considering that curvilinear structure has a well-defined orientation, the confidence in an orientation estimate can also be used as a substitute for detection.

\subsection{Sampling Data}
\label{s:learning_sampling_data}
The final step in our method is to determine how we sample data for the forests. We have two schemes, one for running experiments on training data (e.g. to evaluate parameter options), the other for making final predictions on test data.

In the first scheme, we simply take some fixed size random subsample of pixels across the whole training data, with an equal number of CLS and background pixels (although only the CLS pixels are used for orientation and width prediction). We then take a bootstrap sample of this data to train each tree in the forest. To test the forest, we take a second subsample from the pixels in the main training data not used in the first set. We can repeat this scheme, taking different random subsamples at every iteration, to compute a measure of uncertainty in prediction performance.

To make final predictions on the test data, we adopt a slightly more complicated sampling scheme that aims to better use all the data in the training set. In the first stage, we sample a different random subset of the training data for each tree during forest building, recording which pixels were selected. We then use the forest to predict all the training data, where at each pixel we aggregate only those predictions from trees for which the pixel wasn't selected. We can thus produce an unbiased prediction error at each pixel, analogous to the out-of-bag error described in Breiman's original random forest work~\cite{Breiman_ML01}.

We then build a second forest, where again we sample a different random subset of the training data for each tree, but now rather than uniformly sampling from the data, we weight every sample, $p$, in each image by
%
\begin{equation}
\nu(p) = (1-\alpha) \, \frac{1}{N} +
				    \alpha  \, \frac{E(p)}{ \sum\nolimits_{j=1}^{N} E(p_i) }
\label{e:reweight_sampling}
\end{equation}
%
where there are $N$ pixels in the image, $0 < \alpha < 1$ controls the balance between uniform sampling and oversampling, and $E(p)$ is the prediction error defined in~\eref{e:detection_error} or~\eref{e:orientation_error}.
%
\noindent with .

This has the effect of oversampling pixels that were poorly predicted in the first forest and results in a \comment{statistically?} significant improvement in overall prediction performance. Note that the second stage of this process can be repeated to determine a suitable value for $\alpha$. Indeed subject to time constraints, we could iterate until our predictions in the training data converge. In practice however, we evaluate performance for a fixed set of values for $\alpha$ and select the best.

The resulting forest can then be used to make predictions for all images in the test data.

The complete method for detecting CLS or predicting orientation is summarised in the flow chart in \fref{f:flowchart} \comment{Would this be helpful?}.

\section{Applications \& Datasets}
\label{s:data}
We evaluate our methods on three sets of real data, two of which contain retinograms and the third containing corneal confocal microscopy (CCM) images. These data are described below.

\subsubsection{Synthetic images}
\label{s:dataset_synthetic}

In addition to real data, we use synthetic images in which we have complete control over the ground truth and level of noise in the images. To generate each synthetic image we add a line with elliptical profile to an empty $64\times64$ pixel background. Both the contrast and width of the lines are sampled uniformly in the range $[1, 8]$, whilst line orientation is sampled uniformly over the circle.

The image is then corrupted Rician noise~\cite{missing}, so that for each pixel $p$
%
\begin{equation}
I_{noise}(p) = \sqrt{X^2 + Y^2}
\label{e:rician_noise}
\end{equation}
%
where
%
\begin{align}
X &\sim N(I(p), \eta^2) \\
%
Y &\sim N(0, \eta^2)
\label{e:rician_noise_uv}
\end{align}
%
and $\eta$ controls the level noise. This noise model is signal dependent (as we might expect in real images) but has no spatial correlation.


\subsection{Retinography}
Detecting CLS in retinograms enables us to segment the vessel tree from the background. This is useful for diagnosing or quantifying diseases such as diabetic retinopathy, as it enables us either to focus on the vessels (when measuring venous beading or neovascularization, for example) or to ignore the vessels and focus on the background (when counting microaneurysms, haemorrhages and exudates).

The rate of change of orientation (tortuosity) of blood vessels in a retinogram can serve as a diagnostic indicator of vascular disease such as retinopathy of prematurity~\cite{Wallace_TAOS07,Hart_etal_IJMI99}. Though studies suggest that vessels can be detected and segmented~\cite{Staal_etal_TMI04,Ricci_Perfetti_TMI07}, few have addressed the problem of measuring their orientation and quantifying tortuosity.

\comment{We have more text on the clinical importance of retinograms as a means of diagnosing diabetic retinopathy if deemed relevant}

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.3\columnwidth]{figs/retina/02_test} &
\includegraphics[width=0.3\columnwidth]{figs/retina/17_stare} &
\includegraphics[width=0.3\columnwidth]{figs/fibre/04_fibre_ccm} \\
(a) & (b) & (c) \\
\includegraphics[width=0.3\columnwidth]{figs/retina/02_manual1} &
\includegraphics[width=0.3\columnwidth]{figs/retina/17_stare_v_mask} &
\includegraphics[width=0.3\columnwidth]{figs/fibre/04_fibre_gt} \\
(d) & (e) & (f) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Example images and their ground truth CLS segmentation: (a,d) DRIVE retinograms; (b,e) STARE retinograms; (c,f)Fibre CCM images }
\label{f:image_examples}
\end{figure}
%
\subsubsection{DRIVE retinograms}
\label{s:dataset_drive}
The publicly available DRIVE dataset~\cite{Staal_etal_TMI04} contains 40 full colour, JPEG compressed retinogram images (\fref{f:image_examples}a) that originate from a diabetic neuropathy screening program in The Netherlands, where subjects were aged 25-90. The images were acquired using a Canon CR5 non-mydriatic 3CCD camera with a 45 degree field of view (FOV) and 8 bits per colour plane, and are $768 \by 584$ pixels in size. The field of view is defined by a mask, provided with every image, that results in a cropped image $565 \by 584$ pixels in size.

Forty images from a total of 400 were selected for the dataset, seven of which exhibit signs of mild early diabetic retinopathy. These 40 are split into 20 training and 20 test images. Every image comes with at least one mask (test images have two, of which we use the first), hand-labelled by human observers, that define ground truth vessel segmentations (\fref{f:fig_drive_examples}). The segmentations of the training data produce 565,180 vessel and 5,667,260 background pixels, 565,180 vessel and 5,667,260 background pixels in the test images.

To obtain ground truth orientations, we thin vessel segmentations to a binary skeleton, (which should always be well defined given that CLS are defined to have finite width). We can then compute the local orientation of each skeletonised pixel, and subsequently label any CLS pixel with the orientation of the nearest skeletonised pixel.

Note that where CLS branch or intersect, this orientation is ill-defined. For now, we omit such pixels from the training data. However, we discuss later how this problem may be overcome, and indeed how orientation predictions could actively be used to help detect these features in future work.

\subsubsection{STARE retinograms}
\label{s:dataset_stare}
The second set of data we use is the publicly available STARE set of retinograms, compiled by Hoover et al.~\cite{Hoover_etal_TMI00}. STARE contains 20 images, which unlike DRIVE, are not split into separate training and test sets. The images are digitised from slides acquired from a TopCon TRV-50 fundus camera at 35 field of view. Each image is encoded with 8 bits per colour plane, and are $700 \by 605$ pixels in size. Ten of the images contain pathology.

A field-of-view of approximately 600 pixels in diameter is clearly identifiable in each image, (\fref{f:image_examples}(b)), however definitive masks are not provided with the original data. We create our own masks by setting a threshold of $40$ in the red channel to remove the dark background outside the field-of-view. We note however that Hoover et al. measured performance using the whole image, whilst others have used their own masks without describing exactly how they are generated~\cite{Staal_etal_TMI04} and in some cases, whether masks were used or not is not specified~\cite{missing}. Because the edge of the field-of-view is a common source of false positives, ambiguity in which pixels in this region are included makes a direct comparison to earlier works difficult. The field-of-view masks we use are available from \comment{are we allowed to do this?}.

Two manual segmentations are provided with the images - we use the first observer (labelled as ? in~\cite{Hoover_etal_TMI00}) in our experiments. Combined with our field-of-view masks, the segmentations produce 644,000 vessel and 5,592,658 background pixels.


\subsection{Corneal Confocal Microscopy}
Corneal confocal microscopy (CCM) is a non-invasive technique for imaging small nerve fibres in the cornea. Analysis of these fibres, may allow for early damage to be detected and quantified, thus providing a hugely useful clinical tool in diagnosing and assessing the progress of diabetic neuropathy~\cite{Dabbah_etal_MICCAI10,Dabbah_etal_MIA11}.

Nerve fibres appears a bright CLS in CCM images, as seen \fref{f:image_examples}(c). Usually there is an approximate dominant orientation of fibres within each image, although individual fibres have locally varying orientation, and may branch or join with neighbouring fibres. The fibres have varying width, and in general, are less well-defined than the vessels in the two retinogram datasets.

\subsubsection{Fibre CCM images}
\label{s:dataset_fibre}
We use a set of 200 CCM images, split into a training and test of 100 images each, selected randomly from a larger set of 976 images collected by Dabbah et al.~\cite{Dabbah_etal_MICCAI10}. The images were captured with an HRT-III1 microscope, and are $38\times384$ pixels in size, with a spatial resolution of $1.0417\mu m$ per pixel and encoded on an 8-bit grey scale. The images were acquired from a mixture of normal controls and diabetic patients.

The images have been annotated by experts using CCMetrics~\cite{missing}. This provides an approximate centreline for each fibre, as well as a classification between main and secondary fibres and labelling of branch points. Using the marked centrelines, regardless of fibre type, as CLS ground truth provides 240,154 fibre and 14,352,246 background pixels in the training images, and 314,923 fibre and 14,277,477 background pixels in the test set.


\input{f_synthetic_exp1_pt}

%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_DRIVE_training_cost_sort.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images, all feature vector permutations (features in bold subsequently used to predict test images)}
\label{t:drive_training_c}
\end{table*}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_fibre_training_cost_sort.tex}
\caption{Detecting and predicting the orientation of nerve fibres in confocal corneal microscopy images. Training images, all feature vector permutations (features in bold subsequently used to predict test images)}
\label{t:fibre_training_c}
\end{table*}

\FloatBarrier


%\clearpage
\section{Experiments \& Results}
\label{s:experiments}
%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\multicolumn{3}{c}{Synthetic Dataset} & \multicolumn{2}{c}{DRIVE Dataset} \\
%
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_noise_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_v_width} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_scales_v_width} \\
%
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_RF_noise_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_RF_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_RF_scales_v_width} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_RF_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_RF_scales_v_width} \\
(a) & (b) & (c) & (d) & (e) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Orientation errors for (top) analytic predictions and (bottom) random forest predictions: cumulative distribution function (CDF) of orientation error with respect to (a) noise level and (b) scale on synthetic data; (c) absolute error with respect to line width on synthetic data; (d) CDF of orientation error with respect to scale on DRIVE data; (e) absolute error with respect to vessel width on DRIVE data.}
\label{f:synthetic_exp2}
\end{figure*}
%

\subsection{Synthetic data, increasing noise}
\label{s:experiments_1}
In our first experiments we use a series of synthetic datasets, to each of which we've added an increasing amount of noise. With these we show how the assumptions needed to make accurate predictions of orientation analytically are violated as noise increases. As a result, we show the advantage a learning approach brings and the relevance of this to real data.

For now, we use only the even parts of both Gaussian second derivatives and Gabor filters. \comment{Why?}

For the former we use the three separable filters $\Gxxs$, $\Gyys$ and $\Gxys$, applied across 5 scales, $\sigma \in S=\{1, 2, 4, 8, 16\}$. Using their responses $\Ixxs$, $\Iyys$ and $\Ixys$ together with equation \eref{e:secondderivs_orientation}, we compute the maximum steered response $I_{\hat{\theta}}(\sigma)$. For a given pixel $p$, we can thus define the scale of maximum response as
%
\begin{equation}
\hat{\sigma}_G(p) = \argmax_{\sigma} |I_{\theta}(p ; \sigma)|
\label{e:max_scale_gauss}
\end{equation}
%
and label the associated orientation and response $\hat{\theta}_G(p)$, $\hat{I}_{G}(p)$.

Likewise, for Gabor filters, we use $W_{re}(\theta,\sigma)$, defined in equation \eref{e:gabor_real}, at 5 scales and 6 orientations ($ \sigma = S$, $\theta = \{ \nicefrac{i\pi}{6}\}$), producing image responses $I_{W_{Re}}(p ; \sigma, \theta)$ at pixel $p$.

We can thus define
%
\begin{equation}
\hat{\sigma}_{W}(p), \hat{\theta}_{W}(p) = \argmax_{\sigma, \theta} |I_{W_{Re}}(p ; \sigma, \theta)|
\end{equation}
%
with associated response $\hat{I}_{W}(p)$.

From each set of synthetic data, with noise level $\eta$, we sample sets of $20\,000$ CLS and background pixels, labelled as $\mathfrak{F}(\eta)$ and $\mathfrak{B}(\eta)$ respectively.

In \fref{f:synthetic_exp1} and \fref{f:synthetic_exp2} we show the following:

\begin{itemize}

    \item For $p \in \mathfrak{F}(\eta)$, how often does $\hat{\sigma}_{G}(p) = \sigma$ and $\hat{\sigma}_{W}(p) = \sigma$ as the real line width $w_{gt}(p)$ varies? (\fref{f:synthetic_exp1}a-b).
    %
    \item How do the distributions of $\hat{I}_{G}(p)$ and $\hat{I}_{W}(p)$ compare for $p \in \mathfrak{F}(\eta)$ and $p \in \mathfrak{B}(\eta)$? (\fref{f:synthetic_exp1}c-d).
    %
    \item For $p \in \mathfrak{F}(\eta)$, how often does $\hat{\theta}_{Re}(p) = \theta$ as the $\theta_{gt}(p)$ varies? (\fref{f:synthetic_exp1}e).
    %
    \item For $p \in \mathfrak{F}(\eta)$, how accurate is $\hat{\theta}_G(p)$ as a predictor of $\theta_{gt}(p)$? (\fref{f:synthetic_exp2}a).
    %
\end{itemize}

For the synthetic images with no added noise (\fref{f:synthetic_exp1}, top row), we can see that both sets of filters behave in an ideal manner: that is, \comment{scale (both filters), and orientation (Gabor) sub-bands divide up properly}; the distributions of $\hat{I}_{G}(p)$ and $\hat{I}_{W}(p)$ divide perfectly for CLS and background pixels; and $\hat{\theta}_G(p)$ is a near perfect predictor of $\phi(p)$, with over 95\% of pixels having a prediction error of less than $\pm5^{\deg}$ (\fref{f:synthetic_exp2}a, blue line).

As noise is added to the images, however, this ideal behaviour degenerates. The filter scale that produces maximum response is no longer a reliable indicator of line width (\fref{f:synthetic_exp1}a-b). In terms of separating CLS and background pixels, the distributions of $\hat{I}_{G}$ and $\hat{I}_{W}$ become significantly overlapped (\fref{f:synthetic_exp1}c-d). For Gabor filters, the correct $\hat{\theta}_{W}(p)$ (in the sense that $\hat{\theta}_{Re}(p)$ lies closest to $\phi(p)$), is selected for less than half the pixels, and worse still, the likelihood of the correct band being selected appears to vary with orientation, with lines near to the horizontal or vertical more likely to be produce a correct match (\fref{f:synthetic_exp1}e).

Finally, a direct prediction of orientation using $\hat{\theta}_{G}(p)$ reduces in accuracy, to the point where, for $\eta=3$ only half the pixels have a prediction error less than $\pm20^{\deg}$ (\fref{f:synthetic_exp2}b).

The difficulty in selecting a filter scale based on maximum absolute response in noisy images, leads us to question whether, for analytical methods, applying multiple scales really is beneficial. Examining the error distributions,

\begin{equation}
E_{\hat{\theta}_{G}}(p) = \frac{|\angle(t_{gt}(p) \cdot \hat{t}_{G}^*(p))|}{2}, \; p \in \mathfrak{F}(\eta)
\end{equation}
%
where
%
\begin{alignat}{2}
t_{gt}(p)    &= \cos 2\theta_{gt}(p)    &&+ i\sin 2\theta_{gt}(p) \\
\hat{t}_G(p) &= \cos 2\hat{\theta}_G(p) &&+ i\sin 2\hat{\theta}_G(p)
\end{alignat}
%
and each scale is shown separately along with the original multiscale method, we see that the multiscale is clearly outperformed by at least two of the individual scales ($\sigma=4$ and $\sigma=8$) for data with a noise level of $\eta = 2$ (\fref{f:synthetic_exp2}b). The problem with selecting an individual scale, is that prediction accuracy is naturally biased towards line widths close to the selected filter scale. Considering an estimate of the mean absolute prediction error shown as a function of the true line width, the predictions using a filter with $\sigma=8$ are much more accurate for the widest lines, with a drop off in accuracy between $w(p) = 8$ and $w(p) = 1$ of over $13\deg$ (\fref{f:synthetic_exp2}c).

The specific relationship between the responses at different scales is a product of the non-spatially correlated noise model (which increases the random high-frequency components in the images, leading to disproportionately high responses to the $\sigma = 1$ filters). This is coupled with lines that effectively have infinite length, thus biasing prediction accuracy in favour of large scale filters, regardless of line width, because they have the benefit of smoothing out noise over a larger area without the penalty of including neighbouring structures in their support region.

To show the relevance of these experiments to real data, we repeat each of the above analyses with a set of foreground and background pixels randomly sampled from the retinogram DRIVE training database (\fref{f:retina_exp1} and \fref{f:synthetic_exp2}d-e). We see that the relationship between scales has changed, with the finest scale filter much less likely to produce maximum response and the bias towards large filters for prediction accuracy removed. However, the general problems associated with analytically selecting response and predicting orientation remain, and we again see that in terms of overall errors, we are better off using a single scale (although again, whichever scale we choose will have a performance that is biased towards lines with widths matched to that scale).

We conclude these initial experiments by building our first forests. Using the responses to the Gaussian second derivatives as features, and orientation as output we train forests using all scales, and each scale individually, for each noise level in the synthetic data (using samples from an independently generated set of training images) and for the DRIVE data (using a set of pixels from the training images sampled to have no overlap with the set tested), using the method described in \sref{s:learning_methods} (\fref{f:synthetic_exp2}, bottom).

For the synthetic data, we see how the addition of noise causes a much smaller reduction in prediction accuracy than for the analytic method. Indeed, even with a noise level of $\eta = 3$, the prediction error at the 50th percentile is just $\pm5\deg$ (\fref{f:synthetic_exp2}a). Moreover, forests using all scales in the feature vector outperforms any of forests using a single scale (\fref{f:synthetic_exp2}b), suggesting that by regressing over the training data we have been better able to combine the information across all filter scales (although for the reasons discussed above, using just filters with $\sigma=8$ perform very strongly in these data). \comment{Technically, this last comment is straying into discussion.}

These trends are repeated for the real retinogram data (\fref{f:synthetic_exp2}d-e), with the advantage of including all scales in the feature vectors even more clearly in evidence \comment{arguable}. In particular, the estimation of mean errors in the regressed ``All scales'' predictions as a function of vessel width are flat compared with any of the individual scale, analytic predictions (\fref{f:synthetic_exp2}e).

Thus, we can conclude from these experiments that whilst analytic methods work as intended for images with no or very little noise, with a realistic level of noise the assumptions required to accurately predict orientation or obtain a response that clearly separates CLS and background pixels are invalidated. In such situations, using the same filter responses as input to a random forest regressor will produce significantly more accurate predictions. Moreover, regressing allows us to successfully integrate information across multiple filter scales, thus providing a more consistent prediction for structures of varying width.



\subsection{Comparing Filters banks and Feature Vector Compositions}
\label{s:experiments_2}

Having established a case for regressing orientation, and the advantages of learning methods in general, we now apply random forests to predicting both detection and orientation. In these experiments we seek to test all possible configurations of feature vectors discussed in \sref{s:filtering}.

As a result, for the Gaussian 2nd derivatives we now also apply its Hilbert transform, using the separable filters $H_a$, $H_b$, $H_c$ and $H_d$ (equations \eref{e:secondderivs_hilberta}-\eref{e:secondderivs_hilbertd}). Similarly for Gabor filters, we use both $W_{re}$ and $W_{im}$. To these, we add the \dtcwt{} and Monogenic Signal filtering schemes introduced in \sref{s:filtering_dtcwt} and \sref{s:filtering_monogenic}.

For data, we use the retinal DRIVE and fibre CCM images described in \sref{s:data}\footnote{We have also applied these experiment to the retinal STARE database, however the results are so similar to those of the DRIVE data that have been omitted here. They are available...}. In each case, we use only the training images, from which we randomly sample 10 training and test sets, each of which contain 100,000 CLS and background pixels.

For each set of training data and feature vector combination, we train a random forest classifier to detect CLS and a regressor to predict orientation. Each forest is applied to its respective test set, and the results recorded. For detection, we compute an ROC curve and summarise performance by the area under the curve ($A_z$) whereas we use the median absolute error ($MedAE$) for orientation. The results for each test are presented in \tref{t:drive_training_c} and \tref{t:fibre_training_c}, giving the mean and standard deviation of each test statistic over the 10 repeats.

In addition to $A_z$ and $MedAE$, we compute the time taken to generate a single feature vector for each filter combination. This comprises the time for filtering the image and the time for combining the raw filter responses into a feature vector, and is computed under the assumption that we are extracting feature vectors for all pixels in an image. We also give the total number of dimensions in each vector, which itself will have an effect on the time taken for training and making predictions, and will also effect the total number of feature vectors that can be held in memory at any moment.

\comment{Should we discuss considerations of training set size, and if so, where?}.

Overall, feature vectors composed of Gabor or Gaussian filter outputs proved most successful at the detection task and were both significantly better than the \dtcwt, which in turn was \comment{statistically?} significantly better than the Monogenic Signal. For orientation prediction, there was a clear and \comment{statistically?} significant hierarchy of performance, in the order Gabor, \dtcwt, Gaussian then Monogenic (\tref{missing}).

For the Gaussian filters, steering the coefficients to generate responses equally spaced around the circle benefitted both tasks, with a greater effect for predicting orientation.

For all filter banks, including odd filters along with the even filters proved beneficial, and is particularly necessary for the \dtcwt{} (where neither filter is completely odd or even, thus neither is an ideal match for the majority of structures found in the images). Converting the odd and even responses into a magnitude/phase pair appears to make little difference to detection, but significantly benefits orientation. Similarly, ``folding phase'' (see \sref{s:composing_features_complex}) to make feature vectors invariant to $180\deg$ shifts produces a further improvement in performance for predicting orientation, whilst not affecting the results of detection significantly.

Attempting to make feature vectors rotationally invariant to smaller shifts in angle by circularly permuting responses about the oriented sub-band of maximal response also proved to be of no significant benefit to Gaussian and Gabor filters, whilst it significantly reduced performance for the \dtcwt.

In contrast, pooling neighbourhood responses proved beneficial in all cases, regardless of filter type and combination (note that for reasons of space we have shown $W=1$ and $W=3$ comparisons for just the initial combination of filters, and displayed results for $W=3$ thereafter. However all results are available...). Doing so provides most benefit where only one of an odd/even filter pair is used, particularly so for odd filters which, unless responses are pooled, are a poor match for CLS.

In \sref{s:experiments_1} we showed that including multiple scales in the feature vectors is beneficial when using just the even part of the Gaussian filters, and this result is confirmed here for all filter banks using magnitude/phase features. Adding further scales in between the octave scales for Gaussian and Gabor filters, provides a small benefit for detection and a significant improvement in predicting orientation. Likewise, adding additional oriented sub-bands within each scale also help orientation prediction, although it appears adding additional scales is more beneficial. However, in both cases the size of the resulting feature vectors are too large to make them practically useful on a standard PC (we used a dedicated high-performance computational cluster with 16GB of memory to produce these results).

Similarly, including responses from all feature banks in a single feature vector improves upon the results of any of the filter banks individually, but at an impractical computational cost. Moreover, the improvement in performance is less than adding additional scales to either the Gaussian or Gabor filter banks.

In terms of comparing the two datasets, the trends observed between the different feature vector combinations were the same in both cases. Overall the performance was lower in fibre set. This is likely to be a reflection of the harder nature of the task (noisier images, with less well-defined structures) and the fact that the ground truths annotations show only an approximate centreline for each CLS (and unlike~\cite{Dabbah}, we did not allow any spatial tolerance in the detection task).

\comment{Do we also need to mention: effect of balancing foreground/background sets, number of pixels in training sets, number of trees in forest, stopping parameters in forest, etc? We have results for all of these...}

\subsection{Predicting Real Test Data}
\label{s:experiments_3}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_test_results.tex}
\caption{Test results for the filter banks, applied to the DRIVE, STARE and Fibre datasets. Where appropriate, analytic predictions of orientation are given in brackets.}
\label{t:test_all}
\end{table*}

\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.24\textwidth]{figs/retina/DRIVE_test_detection_roc_zoom} &
\includegraphics[width=0.24\textwidth]{figs/retina/DRIVE_test_orientation_cdf} &
\includegraphics[width=0.24\textwidth]{figs/retina/STARE_detection_roc_zoom} &
\includegraphics[width=0.24\textwidth]{figs/retina/STARE_orientation_cdf} \\
%
(a) & (b) & (c) & (d) \\
%
\noalign{\smallskip}
\end{tabular}
%
\caption{Test results. %
(a) DRIVE detection ROC curves; %
(b) DRIVE orientation prediction error CDFs; %
(c) STARE detection ROC curves; %
(d) STARE orientation prediction error CDFs; %
}
\label{f:retinal results}
\end{figure*}
%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_ret} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_dt_inv} \\
(a) & (b) & (c) & (d) & (e) \\
\noalign{\smallskip}
%
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_ret} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_dt_inv} \\
(f) & (g) & (h) & (i)  & (j) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Detecting vessels in retinography: Best (a-e) and worst (f-j) results in the test set. %
(a,f) original image; %
(b,g) Gabor; %
(c,h) Gaussian; %
(d,i) Monogenic Signal; %
(e,j) \dtcwt; %
}
\label{f:drive_segmentations}
\end{figure*}
%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_ccm} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_dt_inv} \\
(a) & (b) & (c) & (d) & (e) \\
\noalign{\smallskip}
%
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_ccm} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_dt_inv} \\
(f) & (g) & (h) & (i)  & (j) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Detecting fibres in CCM images: Best (a-e) and worst (f-j) results in the test set. %
(a,f) original image; %
(b,g) Gabor; %
(c,h) Gaussian; %
(d,i) Monogenic Signal; %
(e,j) \dtcwt; %
}
\label{f:fibre_segmentations}
\end{figure*}
%
We conclude our experiments by predicting the presence of structure and its associated orientation in the test sets of each of our real datasets. For the DRIVE and fibre data, there are separate training and test sets, and so we can apply the training method directly as described in \sref{s:learning_methods}.

For the STARE data, there is only a single set of 20 images, and thus we run a 10-fold \comment{2 fold at the moment1} cross validation to make an unbiased prediction for each image. To compute out-of-bag errors and thus generate maps of sampling probability for each image, we first predicted CLS and orientation for each filter bank using the forests trained on the DRIVE data.

Following the experiments on the training data described in the previous section, for Gaussian and Gabor filters, and the \dtcwt, we choose the following combinations of features:

\begin{itemize}
  \item Gaussian filters:
  \begin{equation*}
    \{ M_\theta(\sigma), \psi_\theta(\sigma) \}, \; \theta = [ \nicefrac{i\pi}{N_\theta} ] \; N_\theta = 6, \sigma = [1,2,4,8,16]
  \end{equation*}


  \item Gabor filters:
  \begin{equation*}
    \{ M(\theta,\sigma), \psi(\theta,\sigma) \}, \; \theta = [ \nicefrac{i\pi}{N_\theta} ] \; N_\theta = 6, \sigma = [1,2,4,8,16]
  \end{equation*}

  \item \dtcwt{}
    Magnitude, phase; all sub-bands, 5 levels

  \item Monogenic Signal
  \begin{equation*}
    \{ M(\omega), \psi(\omega) \}, \theta(\omega) \; \omega = [2,4,8,16,32]
  \end{equation*}

\end{itemize}

This combination of features was chosen because they provided the best performance on the training data described in the previous section. Note that the option of using additional scales and orientations, or aggregating responses from all filter banks into a single feature vector were not tested here, because as discussed earlier, their computational requirements make them impractical on any standard machine.

The results for these experiments are shown in \tref{t:test_all}. Again, we use summarise detection performance using $A_z$, along with the equal error rate (the average of false positives and true negatives at the operating point at which the two are closest values - equivalent to the intersection of an ROC curve with a diagonal from top left to bottom right). All pixels within the field-of-view of each image in the test data are included in this measure. For orientation we use the median of the absolute prediction error ($mae$) over all CLS pixels. For filter banks where an analytic prediction of orientation can be made, the median absolute error of these predictions is also shown.

Of these data, DRIVE provides the most reliable comparison to earlier work, as both the ground truth and field-of-view masks are well defined and publicly available. On these data, we believe an $A_z$ of 0.967 (achieved using the Gabor filters) currently represents the state-of-the-art, and, using individual image $A_z$ scores as a samples, is significantly higher that the score of 0.962 achieved by Soares et al.~\cite{Soares_etal_TMI06}, which we believe to be the highest score published thus far. For comparison, the original paper by Staal et al.~\cite{Staal_etal_TMI04} achieved an $A_Z$ of 0.952. As Staal et al have made their images publicly available, we have included an ROC for their data, along with a curve for each filter bank using our own method, in \fref{f:retinal_results}(a). We also show the single point of sensitivity and specificity for the second manual annotation of ground truth for the DRIVE data relative to the first. The Gaussian and Gabor curves show that if the respective detection prediction images are thresholded at the appropriate operating point, the resulting binary vessel maps would should as much agreement to the ground truth as the second manual annotation.

For each filtering scheme, the best and worst prediction maps for vessel detection are shown in \fref{f:drive_segmentations}.

Whilst STARE is a publicly available dataset, the images are not supplied with definitive field-of-view masks. Previous works have described using their own masks~\cite{Staal_etal_TMI04}, however as discussed in \sref{s:dataset_stare}, without knowing exactly how these were generated it is difficult to make direct comparisons of results. Using our masks, the Gabor filters again produce the best detection performance ($A_z=0.956$). ROC curves for each filter bank are shown in \fref{f:retinal_results}(b).

Like STARE, the images in the fibre dataset have been used previously, although the images are not publicly available. However, the exact images used to generate the results in~\cite{Dabbah_etal_MICCAI10} and~\cite{Dabbah_etal_MIA11} are unknown and the method for allowing a small spatial tolerance to a true fibre for each positive prediction is not unambiguously described. As a result, we are again unable to make a direct comparison. However, computing a standard ROC with no spatial tolerance allowed, we achieve a detection $A_z$ of 0.917 using Gabor filters. Example fibre detection prediction maps are shown in \fref{f:fibre_segmentations}.

When predicting orientation, we again observe the trends seen in the experiments on the training data. Thus whilst Gabor filters still generate optimal performance, the \dtcwt{} is now superior to Gaussian filtering. The improved accuracy of regressed predictions compared to analytic predictions are also in evidence, both in $mae$ and in the plot of cumulative distribution of error depicted in \fref{f:retinal results}(b,d).

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.48\columnwidth]{figs/retina/ret_vessels_gabor_scales_v_width} &
\includegraphics[width=0.48\columnwidth]{figs/retina/drive_RF_err_v_var} \\
(a) & (b) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{DRIVE data: prediction of orientation as a function of (a) vessel width -- the random forest predictions remain fairly constant at all widths; the analytic predictions are much less accurate for thin vessels; (b) angular variance of orientation prediction -- prediction error consistently reduces as $|t_{est}|$ varies from 0 to 1}
\label{f:drive_gabor_v_width}
\end{figure}

The advantage of learning is further highlighted in \fref{f:drive_gabor_v_width}a, in which an estimate of mean prediction error is shown as a function of vessel width for the regressed and analytic predictions using Gabor filters. For large vessels, both methods provide an accurate prediction of orientation, and as these provide the majority pixels in the data, learning enjoys only a small advantage overall. However, because such vessels are so easily detected, it is trivial to compute an orientation for them from the detection map. Arguably it is at the finest vessels, where the SNR in the images is weakest and detection prediction hardest, that obtaining an accurate orientation prediction is most useful for further processing. In these vessels, the prediction errors diverge and the advantage of regressing is most apparent. Taking, for example, a vessel width of 2.0, the estimate of prediction error for the analytic method is $21.2\deg$ whilst for the regression method it is just $11.5\deg$.

\comment{I've not mentioned the effect of resampling (\ie~out-of-bag classifying the training data, then building a second forest, with weighted sampling given the prediction errors on the training pixels). We can give quantitative results for the difference this makes, and it is also shown pictorially in \fref{f:drive_resampling}.}
%
\begin{figure}[t]
\centering
\begin{tabular}{@{}c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.48\columnwidth]{figs/retina/34_DRIVE_ret} &
\includegraphics[width=0.48\columnwidth]{figs/retina/34_resampling_difference} \\
(a) & (b) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{What effect does resampling have? %
(a) Retinogram; %
(b) Difference between prediction maps with and without resampling. Red indicates a reduced vessel probability with resampling, blue an increased vessel probability. Note the reduced prediction probability at the edge of vessels and for the false predictions caused by pathology in the image; %
}
\label{f:drive_resampling}
\end{figure}
%

%\clearpage
\section{Discussion}
\label{discussion}
%
The experiments in \sref{s:experiments_2} provided an interesting insight into how information from odd and even filters are used by the learning algorithms when detecting CLS or predicting orientation. As noted earlier, using information from both odd and even filters should provide two advantages: it allows for a complete description of structure shape, and thus allows the learning algorithm to better differentiate between CLS and other background structures; it allows a reliable estimation of orientation to be made where noise has disrupted the even symmetry of a CLS. The results provide evidence for this theory, as for all filter banks, in all datasets, using both odd and even filters, rather than even filters alone, improved results for both the detection and orientation prediction task.

For detection, it appears that as long feature vectors contain both parts, it does not matter whether they are kept as an odd/even pair or are combined into magnitude and phase. In contrast, when predicting orientation there is always a clear performance improvement by using magnitude and phase. Indeed, even without the inclusion of phase, magnitude alone is a better feature for orientation prediction than separate odd and even features. This is explained by noting that the magnitude of a response tells us whether some image structure is present at a particular scale and orientation, whilst the phase relays information about the its shape. For predicting orientation, it is the former that provides the most important information, and thus by computing magnitude as a feature we are making the learning algorithms job easier. For detection, both pieces of information are equally important (or else any image structure will be considered as CLS), and thus regardless of whether we use odd/even or magnitude/phase pairs, the classifier must combine two features at each orientation and scale.

In terms of the relative performance of the individual feature banks. As expected, Gabor filters provided the best performance overall. However, this is only true providing: 1) both odd and even filters are used (if only filters even filters are used, it would appear Gaussian second derivatives are better suited to detection, and are certainly more efficient); 2) the full image is filtered at all scales.

When the image is instead downsampled to generate responses for each coarser scale, results using the interpolated responses are significantly worse and are outperformed by the \dtcwt~ when predicting orientation and Gaussian filters when detecting structure. Of course, non-decimating Gabor filters are many orders of magnitude more expensive to compute than any of the other methods, and thus if speed is a critical issue Gaussian filters or the \dtcwt{} may be more suitable. We suggest that in previous work, the negative effect of downsampling or the need to include an odd Gabor filters has often been overlooked~\cite{missing}.

Directional Gaussian second derivative filters were used as features in several landmark CLS detection papers~\cite{Frangi_etal_MICCAI98,Staal_etal_TMI04}. Combined with a modern machine learning algorithm and a well-designed training scheme, they match Gabor filters in producing state-of-the art detection results on the DRIVE database. However, as with Gabor filters, optimal performance is obtained when the second derivative is match with a odd filter. Here we have used its Hilbert transform, as recommended in~\cite{Freeman_Adelson_TPAMI91}. This allowed us to create magnitude/phase pairs from steered responses, which in turn produced significant improvement in predicting orientation. That said, Gaussian filters were still outperformed in this task by Gabor filters and the \dtcwt{} in this task, a result most probably caused by the lack of directional selectivity in the steered responses compared to the filter banks (\eg see \fref{f:oriented_responses} as discussed in \sref{s:filtering_gabor}). Consequently, if Gaussian filters are to be used only for detection, it may be easier (and more efficient) to use the raw responses to the separable filters, and for further simplicity, the Hilbert transform may be replaced by a Gaussian first derivative.

The \dtcwt{} was the most efficient filtering scheme in our tests. Indeed \dtcwt{} features are in the region of 100 times faster to compute than their Gabor equivalents. However, the instability inherent in interpolating decimated responses means they performed worse than Gabor filter in both tasks and worse than Gaussian filters at detecting structure. That said, when the other filters are also decimated, their performance suffers accordingly, and thus if speed is a critical issue, the \dtcwt{} may still be a sensible choice. In addition to speed, a decimating scheme is also preferable in cases where a full decomposition of the image across scale and orientation cannot be stored in working memory. During training, full feature vectors can be sampled from the decimated transform as and when needed (and typically we will only sample a small percentage of the pixels in each image). Prediction can then be carried out by sampling blockwise features in a test image. Whilst a similar blockwise processing scheme is possible for full filtering, it is more complicated and less efficient to compute, as overlapping border pixels must be included for each block.

Unlike the \dtcwt{}, there is little reason to recommend ever using the Monogenic Signal as a filtering scheme if the objects of interest in an image are CLS. As noted in section X, because the even filter is isotropic, we cannot accurately estimate the orientation of even symmetric structures (although pooling neighbourhood responses in a regressor helps somewhat, performance here still lags behind the other method). As a detector, the performance is limited by the fact that magnitude and phase are extracted in one direction only at each scale. Thus a blob cannot be properly differentiated from a CLS. This effect can be seen in the detection prediction maps of vessels shown in \fref{f:drive_segmentations}(e,j) and is particularly clear in the CCM images, where bright blobs are a feature of the background that should not be mistaken for fibres (\fref{f:fibre_segmentations}(e,j)).

\subsection{Rotational Invariance}
Perhaps one of the more surprising results we have shown is the lack of benefit in rearranging responses to generate theoretically more rotationally invariant features. That this should have a negative effect on the \dtcwt{} is not surprising, given the lack of rotational symmetry between the sub-band responses (see \fref{f:oriented_responses}(c)). However, at first glance, it seems strange such a scheme does not benefit the Gaussian and Gabor filters, both of which have perfect rotational symmetry between their oriented sub-bands (\fref{f:oriented_responses}(a,b)). We think there are two factors behind this result. Firstly, whilst making a feature vector rotationally invariant to up to some discrete orientation shift should produce a more compact feature space, it does not necessarily follow that the resulting space is any easier for the learning algorithm to separate CLS and background pixels. Secondly, consider again our original experiments in \sref{s:experiments_1}, and particularly the plots shown in \fref{f:synthetic_exp1}(u-y). When there is noise in the image, for a significant percentage of pixels (potentially as many as 40\%), the oriented sub-band that produces maximum response will not correspond to the true orientation of the structure. Thus we may be rearranging the feature space without actually compacting it in any meaningful way. In either case, it seems that provided we have sufficient data (and as we are sub-sampling pixels from a sets of images this is rarely a problem), and a flexible learning algorithm like a random forest, then there is no need to try and create rotationally invariant features.

\subsection{Pooling}
Whereas creating rotationally invariant features is a theoretically elegant trick that doesn't actually appear to offer practical benefit, pooling neighbourhood responses is a simple idea that always improved performance. The only downside to sampling a $3\times3$ neighbourhood of responses is the 9-fold increase the dimensions of the feature vectors. However, provided it is possible to sample enough pixels to populate the increased size of the feature space, we recommend pooling responses should always be used.

As discussed in section \sref{s:learning_forest}, when constructing trees, at each leaf node we are left with a sample of the training data output labels. For orientation, our approach is to replace this sample with summary statistics -- the angular mean and variance, computed by taking mean of the complex vectors $t_{gt}(p_i)$. When we further compile the leaf outputs from each tree in the forest, we repeat the process, to estimate an overall prediction of angular mean and variance.

Thus far, we have only used the angular mean in our results, and have shown how this makes for a more accurate prediction of structure orientation than analytic predictions. However, the \emph{angular dispersion} (Equation \eref{e:angular_dispersion}) of the Random Forest predictions may also be a useful value in further processing.

The dispersion can be seen as a confidence in the forest's prediction: a test pixel with feature vector well matched to a pattern in the training data, will be associated with a set of leaf samples that have a very similar direction, and so the dispersion (\ie~the magnitude of the mean direction vectors) will be close to one; in contrast, a feature vector not matched to a consistent pattern in the training data will be associated with leaf samples randomly distributed about the circle, the mean of which will have a magnitude close to zero.

This relationship is confirmed in \fref{f:drive_gabor_v_width}b, in which the mean error in orientation prediction is shown as a function of the predicted angular dispersion, for each filter bank applied to the DRIVE data. Note the consistent reduction in angular error as the dispersion moves from 0 to 1 for all feature types.

This relationship is explored further in \fref{f:first_pic}(c), in which we show orientation predictions for a retinogram in the DRIVE data, with colour hue representing the predicted angle at each each pixel, and intensity proportional to the predicted angular dispersion.

We can see that while angular dispersion is generally high at vessels (indeed as a classifier it achieves $A_z = 0.901$) it is much lower at bifurcations and crossings where orientation is not well defined, and thus provides information in addition to the detection probabilities predicted by the Random Forest classifier. For example, where angular measurements are of direct interest (\eg~vessel tortuosity~\cite{Hart_etal_IJMI99}) the detection predictions may suggest which pixels to include in the analysis with orientation confidence weighting the inputs. Thus the orientation confidence could sensibly down-weight the contribution of points where orientation is ill-defined even though vessel probability may be high.

Alternatively, in further processing of detection probabilities such as using a tracking algorithm to group connected pixels, the confidence in orientation prediction could be used to control the spread of potential paths, with a narrow focus along CLS and a wide spread at bifurcations where the paths necessarily diverge.

Continuing this idea, we note that summarising the leaf samples by their angular mean and dispersion is akin to assuming they form a single unimodal distribution of orientations about the circle. However, we can also choose to fit a bimodal (or higher order) distributions, that can better model the samples obtained at branching points. Indeed, an approach that has found success when using Random Forest regressors to locate objects, is to sample predictions directly from the leaf samples, thus avoiding having to make any assumptions about their distribution. Thus in a tracking algorithm alluded to above, at each step we could directly sample the direction of our next step from the leaf distributions of training pixels similar to our current location, which may provide an elegant and efficient way of navigating bifurcations. This approach is more memory intensive (because we need to store all training samples, rather than replacing them with summary statistics) but would be an interesting avenue for further research.

Overall, it is pleasing to note that with a principled selection of filters and feature vectors, and an up-to-date learning algorithm, it is possible to generate state-of-the-art results for a variety of data, without having to make handcrafted solutions specific to any one set of images. The method described here may be applied to any images in which CLS exist and for which we have ground truth.

\section{Conclusions}
\label{conclusions}
\comment{Rewrite these when have decided on the final content of the paper}

Combining suitable filters with Random Forests produces vessel segmentation that matches the state of the art without application specific post-processing as used in rival methods (and that we would expect to improve results further).
Moreover, we have shown that regressing orientation estimates using similar machinery is more accurate than relying on analytical estimations.
Though demonstrated on retinograms, our methods are generally applicable to linear structures in any images where ground truth is available.
Most promisingly, we note the larger improvement in orientation estimation for particularly challenging structures such as thin, low-contrast vessels. As a further advantage of regressing with random forests, we propose that both the predicted orientation \emph{and} its associated magnitude may be useful features in further processing.

\comment{We must, however, take care when building regressors for orientation prediction in order to ensure that angles wrap around the circle correctly.}


\section*{Acknowledgements}
We thank Nick Kingsbury for the \dtcwt{} Matlab toolbox. Mammograms were provided by the Nightingale Breast Centre, South Manchester University Hospitals Trust, UK and were annotated by Dr Caroline Boggis and Dr Rumana Rahim. This work was funded by EPSRC grant EP/E031307/1.

%\input{additional_tables.tex}

%
\bibliographystyle{plain}
\bibliography{%
./bib/_aliases,%
./bib/mobio,%
./bib/mammography,%
./bib/ml,%
./bib/nailfold,%
./bib/papers_by_year,%
./bib/local}

\end{document} 